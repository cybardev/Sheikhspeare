{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYOd5pL9tQHJ"
      },
      "source": [
        "# Cy | gist\n",
        "\n",
        "## Summarizer model based on Google's Flan-T5\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/cybardev/Sheikhspeare/blob/main/sheikhspeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGa0q57dtO46"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAiaIQ5N2-FE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Define the model name and load the tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLom68x03Gw8"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Print out which device we're using (GPU or CPU)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0UwUj2693HI"
      },
      "outputs": [],
      "source": [
        "def raw_generator(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkEa8vmo9F3p"
      },
      "outputs": [],
      "source": [
        "# Define a sample text for conversion\n",
        "sample_text = \"\"\"\n",
        "Person A: Hey, did you hear about the new project management software our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I heard a bit about it. What’s the deal with it?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" The management thinks it’s going to streamline our workflow, especially with remote teams. It’s supposed to integrate all the tools we use, like Slack, Trello, and Google Drive, into one platform.\n",
        "\n",
        "Person B: That sounds interesting. But I’m a bit concerned about the learning curve. Is it user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen, it looks pretty intuitive. They’re also planning to run a couple of training sessions to get everyone up to speed. The first one is next Monday.\n",
        "\n",
        "Person B: Okay, that helps. I guess I’ll have to attend that session. How does it compare to what we’re using now?\n",
        "\n",
        "Person A: It’s supposed to be much more efficient. We’ll be able to track project progress more easily and get real-time updates. Plus, it has built-in analytics to help us with performance tracking.\n",
        "\n",
        "Person B: That sounds promising. I just hope it doesn’t come with too many bugs at launch.\n",
        "\n",
        "Person A: Yeah, that’s always a concern with new software. But they’ve been testing it for a while now, so fingers crossed it goes smoothly.\n",
        "\n",
        "Person B: Let’s hope for the best. Thanks for the info!\n",
        "\n",
        "Person A: No problem. See you at the training!\n",
        "\"\"\"\n",
        "\n",
        "# Convert the sample text using the pre-trained model (without fine-tuning)\n",
        "pre_finetuned_summary = raw_generator(sample_text)\n",
        "print(\"Summary before fine-tuning:\", pre_finetuned_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLYP32_U3Mrx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load relevant dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN3wAFL93lSh"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing subsets\n",
        "dataset_split = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Further reduce the training set size for faster testing during development\n",
        "small_train_dataset = dataset_split['train'].train_test_split(test_size=0.99)['train']\n",
        "eval_dataset = dataset_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGBH0IW93qQS"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "  # Extract the articles from the dataset\n",
        "  inputs = [doc for doc in examples['article']]\n",
        "\n",
        "  # Tokenize the articles (inputs) with padding and truncation to a max length of 512\n",
        "  model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Tokenize the conversions (labels) using the target tokenizer context\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(examples['highlights'], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Attach the tokenized conversions as labels to the model inputs\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7FADB7W4_eE"
      },
      "outputs": [],
      "source": [
        "# Tokenize the small training dataset\n",
        "tokenized_train_dataset = small_train_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Tokenize the evaluation dataset\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqVonjlJ5xfe"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',              # Directory to save the model checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of every epoch\n",
        "    learning_rate=2e-5,                  # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    weight_decay=0.01,                   # Regularization to prevent overfitting\n",
        "    save_total_limit=3,                  # Only keep the last 3 checkpoints\n",
        "    num_train_epochs=4,                  # Number of training epochs\n",
        "    predict_with_generate=True,          # Enable text generation during evaluation\n",
        "    logging_dir=\"./logs\"                 # Directory for storing training logs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FB7tuyi6UCh"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Create the trainer object\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                            # The model to be trained\n",
        "    args=training_args,                     # The training arguments defined earlier\n",
        "    train_dataset=tokenized_train_dataset,  # The tokenized training dataset\n",
        "    eval_dataset=tokenized_eval_dataset,    # The tokenized evaluation dataset\n",
        "    tokenizer=tokenizer                     # The tokenizer to handle input and output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fP00IFjb6b76"
      },
      "outputs": [],
      "source": [
        "# Let's train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rk-oMbY6u6e"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the evaluation dataset\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPmWE_dw60E5"
      },
      "outputs": [],
      "source": [
        "def tuned_generator(text):\n",
        "  # Tokenize the input text and move it to the correct device\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "  # Generate the converted text using the fine-tuned model\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode the generated conversion back into text and return it\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBLRVAnv69su"
      },
      "outputs": [],
      "source": [
        "print(tuned_generator(\"\"\"\n",
        "Person A: Hey, did you hear about the new project management software our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I heard a bit about it. What’s the deal with it?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" The management thinks it’s going to streamline our workflow, especially with remote teams. It’s supposed to integrate all the tools we use, like Slack, Trello, and Google Drive, into one platform.\n",
        "\n",
        "Person B: That sounds interesting. But I’m a bit concerned about the learning curve. Is it user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen, it looks pretty intuitive. They’re also planning to run a couple of training sessions to get everyone up to speed. The first one is next Monday.\n",
        "\n",
        "Person B: Okay, that helps. I guess I’ll have to attend that session. How does it compare to what we’re using now?\n",
        "\n",
        "Person A: It’s supposed to be much more efficient. We’ll be able to track project progress more easily and get real-time updates. Plus, it has built-in analytics to help us with performance tracking.\n",
        "\n",
        "Person B: That sounds promising. I just hope it doesn’t come with too many bugs at launch.\n",
        "\n",
        "Person A: Yeah, that’s always a concern with new software. But they’ve been testing it for a while now, so fingers crossed it goes smoothly.\n",
        "\n",
        "Person B: Let’s hope for the best. Thanks for the info!\n",
        "\n",
        "Person A: No problem. See you at the training!\n",
        "\"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Publish the model\n",
        "from google.colab import userdata\n",
        "model.save_pretrained(\n",
        "    \"cybargist\",\n",
        "    push_to_hub=True,\n",
        "    repo_name=\"cybargist\",\n",
        "    use_auth_token=userdata.get(\"HF_TOKEN\")\n",
        ")"
      ],
      "metadata": {
        "id": "oM8fTznmJqem"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}